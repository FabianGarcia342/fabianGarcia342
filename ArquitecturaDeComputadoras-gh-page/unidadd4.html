<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Menú Desplegable</title>
        <link rel="stylesheet" href="style.css">
      </head>
<body>
    <!-- Barra de navegación con menú desplegable -->
<div class="navbar">
    <a href="index.html">Inicio</a>
    <div class="dropdown">
      <button class="dropbtn">Temario
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <a href="unidad1.html">Unidad 1</a>
        <a href="unidad2.html">Unidad 2</a>
        <a href="unidad3.html">Unidad 3</a>
        <a href="unidadd4.html">Unidad 4</a>
      </div>
    </div>
    <a href="#contacto">Contacto</a>
  </div>

    <header class= "header">

        <div class="header-content container">
            
            <h1> unidad 4 </h1>
            <h3>Aspectos basicos de la computacion paralela</h1>
            
                <p>4.1 Modelos de arquitectura de computo.</p>
                <p>La computación paralela es una forma de cómputo en la que muchas instrucciones se ejecutan simultáneamente, operando sobre el principio de que problemas grandes, a menudo se pueden dividir en unos más pequeños, que luego son resueltos simultáneamente (en paralelo). Hay varias formas diferentes de computación paralela: paralelismo a nivel de bit, paralelismo a nivel de instrucción, paralelismo de datos y paralelismo de tareas. El paralelismo se ha empleado durante muchos años, sobre todo en la computación de altas prestaciones, pero el interés en ella ha crecido últimamente debido a las limitaciones físicas que impiden el aumento de la frecuencia. Como el consumo de energía y por consiguiente la generación de calor de las computadoras constituye una preocupación en los últimos años, la computación en paralelo se ha convertido en el paradigma dominante en la arquitectura de computadores, principalmente en forma de procesadores multinúcleo.</p>
                <p>Los programas informáticos paralelos son más difíciles de escribir que los secuenciales, porque la concurrencia introduce nuevos tipos de errores de software, siendo las condiciones de carrera los más comunes. La comunicación y sincronización entre diferentes subtareas son algunos de los mayores obstáculos para obtener un buen rendimiento del programa paralelo. La máxima aceleración posible de un programa como resultado de la paralelización se conoce como la ley de Amdahl.</p>
                <h4>Ley de Amdahl y Ley de Gustafson</h4>
                <p>Idealmente, la aceleración a partir de la paralelización es lineal, doblar el número de elementos de procesamiento debe reducir a la mitad el tiempo de ejecución y doblarlo por segunda vez debe nuevamente reducir el tiempo a la mitad. Sin embargo, muy pocos algoritmos paralelos logran una aceleración óptima. La mayoría tienen una aceleración casi lineal para un pequeño número de elementos de procesamiento, y pasa a ser constante para un gran número de elementos de procesamiento.</p>
                <p>La aceleración potencial de un algoritmo en una plataforma de cómputo en paralelo está dada por la ley de Amdahl, formulada originalmente por Gene Amdahl en la década de 1960. Esta señala que una pequeña porción del programa que no pueda paralelizarse va a limitar la aceleración que se logra con la paralelización.</p>
                <p>La ley de Gustafson es otra ley en computación que está en estrecha relación con la ley de Amdahl. Ambas leyes asumen que el tiempo de funcionamiento de la parte secuencial del programa es independiente del número de procesadores. La ley de Amdahl supone que todo el problema es de tamaño fijo, por lo que la cantidad total de trabajo que se hará en paralelo también es independiente del número de procesadores, mientras que la ley de Gustafson supone que la cantidad total de trabajo que se hará en paralelo varía linealmente con el número de procesadores.</p>
                <h4>Condiciones de carrera, exclusión mutua, sincronización, y desaceleración paralela.</h4>
                <p>Las subtareas en un programa paralelo a menudo son llamadas hilos. Algunas arquitecturas de computación paralela utilizan versiones más pequeñas y ligeras de hilos conocidas como hebras, mientras que otros utilizan versiones más grandes conocidos como procesos. Sin embargo, «hilos» es generalmente aceptado como un término genérico para las subtareas. Los hilos a menudo tendrán que actualizar algunas variables que se comparten entre ellos. Las instrucciones entre los dos programas pueden entrelazarse en cualquier orden.</p>
                <h4>Dependencias</h4>
                <p>Entender la dependencia de datos es fundamental en la implementación de algoritmos paralelos. Ningún programa puede ejecutar más rápidamente que la cadena más larga de cálculos dependientes (conocida como la ruta crítica), ya que los cálculos que dependen de cálculos previos en la cadena deben ejecutarse en orden. Sin embargo, la mayoría de los algoritmos no consisten sólo de una larga cadena de cálculos dependientes; generalmente hay oportunidades para ejecutar cálculos independientes en paralelo.</p>
                <h4>Grado de paralelismo:</h4>
                <p>Muy grueso: Programas.</p>
                <p>Grueso: Subprogramas, tareas.</p>
                <p>Fino: Instrucción.</p>
                <p>Muy fino: Fases de instrucción.</p>
                <h4>Modelos de consistencia.</h4>
                <p>Los lenguajes de programación en paralelo y computadoras paralelas deben tener un modelo de consistencia de datos también conocido como un modelo de memoria. El modelo de consistencia define reglas para las operaciones en la memoria del ordenador y cómo se producen los resultados.</p>
                <p>Uno de los primeros modelos de consistencia fue el modelo de consistencia secuencial de Leslie Lamport. La consistencia secuencial es la propiedad de un programa en la que su ejecución en paralelo produce los mismos resultados que un programa secuencial.</p>
                <h4>Single Instruction, Single Data (SISD)</h4>
                <p>Hay un elemento de procesamiento, que tiene acceso a un único programa y a un almacenamiento de datos. En cada paso, el elemento de procesamiento carga una instrucción y la información correspondiente y ejecuta esta instrucción. El resultado es guardado de vuelta en el almacenamiento de datos. Luego SISD es el computador secuencial convencional, de acuerdo al modelo de von Neumann.

                </p>
                <h4>Multiple Instruction, Single Data (MISD)</h4>
                <p>Hay múltiples elementos de procesamiento, en el que cada cual tiene memoria privada del programa, pero se tiene acceso común a una memoria global de información. En cada paso, cada elemento de procesamiento de obtiene la misma información de la memoria y carga una instrucción de la memoria privada del programa. Este modelo es muy restrictivo y no se ha usado en ningún computador de tipo comercial.</p>
                <h4>Single Instruction, Multiple Data (SIMD)</h4>
                <p>Hay múltiples elementos de procesamiento, en el que cada cual tiene acceso privado a la memoria de información (compartida o distribuida). Sin embargo, hay una sola memoria de programa, desde la cual una unidad de procesamiento especial obtiene y despacha instrucciones. En cada paso, cada unidad de procesamiento obtiene la misma instrucción y carga desde su memoria privada un elemento de información y ejecuta esta instrucción en dicho elemento. Para aplicaciones con un grado significante de paralelismo de información, este acercamiento puede ser muy eficiente.</p>
                <h4>Multiple Instruction, Multiple Data (MIMD)</h4>
                <p>Hay múltiples unidades de procesamiento, en la cual cada una tiene tanto instrucciones como información separada. Cada elemento ejecuta una instrucción distinta en un elemento de información distinto. Los elementos de proceso trabajan asíncronamente. Los clusters son ejemplo son ejemplos del modelo MIMD.</p>

            </p>4.2 Modelos de arquitectura de computo.
            <h4>Paralelismo a nivel de bit</h4>
            <p>Desde el advenimiento de la integración a gran escala (VLSI) como tecnología de fabricación de chips de computadora en la década de 1970 hasta alrededor de 1986, la aceleración en la arquitectura de computadores se lograba en gran medida duplicando el tamaño de la palabra en la computadora, la cantidad de información que el procesador puede manejar por ciclo.</p>
            <p>Históricamente, los microprocesadores de 4 bits fueron sustituidos por unos de 8 bits, luego de 16 bits y 32 bits, esta tendencia general llegó a su fin con la introducción de procesadores de 64 bits, lo que ha sido un estándar en la computación de propósito general durante la última década.</p>
            <h4>Paralelismo a nivel de instrucción</h4>
            <p>Los procesadores modernos tienen ''pipeline'' de instrucciones de varias etapas. Cada etapa en el pipeline corresponde a una acción diferente que el procesador realiza en la instrucción correspondiente a la etapa; un procesador con un pipeline de N etapas puede tener hasta n instrucciones diferentes en diferentes etapas de finalización.</p>
            <h4>Paralelismo de datos</h4>
            <p>El paralelismo de datos es el paralelismo inherente en programas con ciclos, que se centra en la distribución de los datos entre los diferentes nodos computacionales que deben tratarse en paralelo. Muchas de las aplicaciones científicas y de ingeniería muestran paralelismo de datos.</p>
            <p>Una dependencia de terminación de ciclo es la dependencia de una iteración de un ciclo en la salida de una o más iteraciones anteriores. Las dependencias de terminación de ciclo evitan la paralelización de ciclos.</p>
            <h4>Paralelismo de tareas</h4>
            <p>Es un paradigma de la programación concurrente que consiste en asignar distintas tareas a cada uno de los procesadores de un sistema de cómputo. En consecuencia, cada procesador efectuará su propia secuencia de operaciones. En su modo más general, el paralelismo de tareas se representa mediante un grafo de tareas, el cual es subdividido en subgrafos que son luego asignados a diferentes procesadores.</p>

            <p>>4.2.1 CLasificacion.</p>
            <p>Las computadoras paralelas se pueden clasificar de acuerdo con el nivel en el que el hardware soporta paralelismo. Esta clasificación es análoga a la distancia entre los nodos básicos de cómputo.
            </p>
            <p> Computación multinúcleo: un procesador multinúcleo es un procesador que incluye múltiples unidades de ejecución (núcleos) en el mismo chip.</p>
            <p>Multiprocesamiento simétrico: un multiprocesador simétrico (SMP) es un sistema computacional con múltiples procesadores idénticos que comparten memoria y se conectan a través de un bus.

            </p>
            <p>Computación en clúster: un clúster es un grupo de ordenadores débilmente acoplados que trabajan en estrecha colaboración, de modo que en algunos aspectos pueden considerarse como un solo equipo.</p>
            <p>Procesamiento paralelo masivo: tienden a ser más grandes que los clústeres, con «mucho más» de 100 procesadores.</p>
            <p>Computación distribuida: la computación distribuida es la forma más distribuida de la computación paralela. Se hace uso de ordenadores que se comunican a través de la Internet para trabajar en un problema dado.</p>
            <p>Circuitos integrados de aplicación específica: debido a que un ASIC (por definición) es específico para una aplicación dada, puede ser completamente optimizado para esa aplicación.</p>
            <p>Procesadores vectoriales: pueden ejecutar la misma instrucción en grandes conjuntos de datos.</p>
            
            <p> 4.2.2 Arquitectura de Computadoras Secuenciales.</p>
            <p>A diferencia de los sistemas combinacionales, en los sistemas secuenciales, los valores de las salidas, en un momento dado, no dependen exclusivamente de los valores de las entradas en dicho momento, sino también dependen del estado anterior o estado interno.</p>
            <p>El sistema secuencial requiere de la utilización de un dispositivo de memoria que pueda almacenar la historia pasada de sus entradas (denominadas variables de estado) y le permita mantener su estado durante algún tiempo, estos dispositivos de memoria pueden ser sencillos como un simple retardador o celdas de memoria de tipo DRAM, SRAM o multivibradores biestables también conocido como Flip-Flop.</p>
            <h4>Tipos de sistemas secuenciales
            </h4>
            <p>En este tipo de circuitos entra un factor que no se había considerado en los circuitos combinacionales, dicho factor es el tiempo, según como manejan el tiempo se pueden clasificar en: circuitos secuenciales síncronos y circuitos secuenciales asíncronos.</p>
            <h4>Circuitos secuenciales asíncronos</h4>
            <p>En circuitos secuenciales asíncronos los cambios de estados ocurren al ritmo natural asociado a las compuertas lógicas utilizadas en su implementación, lo que produce retardos en cascadas entre los biestables del circuito, es decir no utilizan elementos especiales de memoria, lo que puede ocasionar algunos problemas de funcionamiento, ya que estos retardos naturales no están bajo el control del diseñador y además no son idénticos en cada compuerta lógica.</p>
            <h4>Circuitos secuenciales síncronos</h4>
            <p>Los circuitos secuenciales síncronos solo permiten un cambio de estado en los instantes marcados o autorizados por una señal de sincronismo de tipo oscilatorio denominada reloj (cristal o circuito capaz de producir una serie de pulsos regulares en el tiempo), lo que soluciona los problemas que tienen los circuitos asíncronos originados por cambios de estado no uniformes dentro del sistema o circuito.</p>
            4.2.3 De multiprocesamiento.
        </p>
        <p>4.2 Modelos de arquitectura de computo.
            <p> 4.2.1 CLasificas.</p>
            <h4>PARALELISMO A NIVEL DE BIT</h4>
            <p>El paralelismo a nivel de bit se refiere a la capacidad de un sistema o procesador para realizar
                múltiples operaciones en paralelo, de manera simultánea en diferentes bits de un registro o en
                diferentes bits de múltiples registros.
                
                <p>EXPLICACION:</p>
                
                Desde el advenimiento de la integración a gran escala (VLSI) como tecnología de fabricación
                de chips de computadora en la década de 1970 hasta alrededor de 1986, la aceleración en la
                arquitectura de computadores se lograba en gran medida duplicando el tamaño de la palabra
                en la computadora, la cantidad de información que el procesador puede manejar por ciclo. El
                aumento del tamaño de la palabra reduce el número de instrucciones que el procesador debe
                ejecutar para realizar una operación en variables cuyos tamaños son mayores que la longitud
                de la palabra.</p>
                <p>En otras palabras, el paralelismo a nivel de bit se refiere a la habilidad de procesar varias
                    operaciones de manera simultánea, en vez de realizarlas de manera secuencial. Por
                    ejemplo, si se tienen dos números de 8 bits que se deben sumar, un procesador que
                    cuenta con paralelismo a nivel de bit puede realizar la suma en un solo ciclo de reloj, en
                    lugar de realizar la suma bit a bit en ocho ciclos de reloj.</p>
                    <p>Este tipo de paralelismo es comúnmente utilizado en procesadores modernos, ya que
                        permite una mayor velocidad y eficiencia en la ejecución de tareas, especialmente en
                        tareas que requieren procesamiento de grandes cantidades de datos, como en
                        aplicaciones de gráficos, video y procesamiento de señales digitales.</p>
            4.2.3 De multiprocesamiento.
            <p>Un multiprocesador puede verse como un computador paralelo compuesto por
                varios procesadores interconectados que comparten un mismo sistema de
                memoria.</p>
                <p>Dado que los multiprocesadores comparten diferentes módulos de memoria,
                    pudiendo acceder a un mismo módulo varios procesadores, a los
                    multiprocesadores también se les llama sistemas de memoria compartida.</p>
        </p>
       <p>
            4.4 Unidad Central de Procesamiento.
            <p>CPU es la abreviatura de Central Processing Unit (Unidad Central de Procesamiento en español) que se encuentra en un ordenador. Pero también se puede hablar simplemente de procesador. El procesador es el componente de hardware central y, por tanto, la base del ordenador. Sin ella, un ordenador no puede funcionar en absoluto. Esto se debe principalmente a que la CPU es responsable de todos los cálculos necesarios para que funcione.</p>
            <p>Para entender la importancia de la CPU, hay que tener conocimientos sobre el funcionamiento básico de un ordenador. Los cálculos del ordenador se llevan a cabo mediante los comandos de máquina, que se pueden considerar como instrucciones para el procesador. Cabe señalar que estos comandos pueden representarse con el código binario, es decir, como secuencias de ceros y unos. Esto es justamente lo que ocurre en el ordenador, ya que la CPU solo puede procesar instrucciones en código binario.</p>
            <p>4.4.1 Unidad Aritmética Lógica.</p>
            <p>En computación, la unidad aritmético
                lógica, también conocida como ALU (siglas
                en inglés de arithmetic logic unit), es un
                circuito digital que calcula operaciones
                aritméticas (como suma, resta,
                multiplicación, etc.) y operaciones lógicas
                (si, y, o, no), entre dos números</p>
                <p>Muchos tipos de circuitos electrónicos
                    necesitan realizar algún tipo de operación
                    aritmética, así que incluso el circuito dentro
                    de un reloj digital tendrá una ALU
                    minúscula que se mantiene sumando 1 al
                    tiempo actual, y se mantiene comprobando
                    si debe activar el pitido del temporizador,
                    etc.</p>
                    <p>Por mucho, los más complejos circuitos
                        electrónicos son los que están construidos
                        dentro de los chips de microprocesadores
                        modernos como el Intel Core i7 o el Phenom II. Por lo tanto, estos procesadores tienen dentro de ellos un ALU muy
                        complejo y potente. De hecho, un microprocesador moderno (y los mainframes) pueden tener múltiples núcleos,
                        cada núcleo con múltiples unidades de ejecución, cada una de ellas con múltiples ALU.</p>
                        <p>Muchos otros circuitos pueden contener en el interior una unidad aritmético lógica: unidades de procesamiento
                            gráfico como las que están en las GPU NVIDIA y AMD, FPU como el viejo coprocesador matemático 80387, y
                            procesadores digitales de señales como los que se encuentran en tarjetas de sonido Sound Blaster, lectoras de CD y
                            los televisores de alta definición. Todos éstos tienen en su interior varias ALU potentes y complejas.</p>
                    </p>
       <p>4.5 Registros.</p>
       <p>Los registros de propósito general son los registros que suelen utilizarse como operandos en las instrucciones del ensamblador. Estos registros se pueden asignar a funciones concretas: datos o direccionamiento. En algunos procesadores todos los registros se pueden utilizar para todas las funciones.</p>
       <p>Los registros de datos se pueden diferenciar por el formato y el tamaño de los datos que almacenan; por ejemplo, puede haber registros para números enteros y para números en punto flotante.</p>
       <p>Los registros de direccionamiento se utilizan para acceder a memoria y pueden almacenar direcciones o índices. Algunos de estos registros se utilizan de manera implícita para diferentes funciones, como por ejemplo acceder a la pila, dirigir segmentos de memoria o hacer de soporte en la memoria virtual.</p>
       <h4>Registros de instrucción</h4>
       <p>Los dos registros principales relacionados con el acceso a las instrucciones son:
    </p>
       <p>Program counter (PC): registro contador del programa, contiene la dirección de la instrucción siguiente que hay que leer de la memoria</p>
       <p>Instruction register (IR): registro de instrucción, contiene la instrucción que hay que ejecutar.</p>
            <h4>Registros de acceso a memoria</h4>
            <p>Memory address register (MAR): registro de direcciones de memoria, donde ponemos la dirección de memoria a la que queremos acceder.</p>
            <p>Memory buffer register (MBR): registro de datos de memoria; registro donde la memoria deposita el dato leído o el dato que queremos escribir.</p>
            <h4>Registros de estado y de control</h4>
            <p>La información sobre el estado del procesador puede estar almacenada en un registro o en más de uno, aunque habitualmente suele ser un único registro denominado registro de estado.
            </p>
            <p>Los bits del registro de estado son modificados por el procesador como resultado de la ejecución de algunos tipos de instrucciones, por ejemplo instrucciones aritméticas o lógicas, o como consecuencia de algún acontecimiento, como las peticiones de interrupción. Estos bits son parcialmente visibles para el programador, en algunos casos mediante la ejecución de instrucciones específicas.
            </p>
        
        </div>
    </header>
   
    <BUtton type="button" class="boton">
        <a href="unidad3.html"> pagina anterior</a>

    </BUtton>
    <button type="button" class="boton">
        <a href="index.html">pagina principal</a>

    </button>
</body>
</html>